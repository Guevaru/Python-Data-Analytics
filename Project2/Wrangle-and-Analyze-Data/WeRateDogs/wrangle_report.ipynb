{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrangle Report"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting what I learned in the Data Wrangling Data Course, a component of the Udacity Data Analysis Nanodegree program, into practice is the goal of this project.\n",
    "The dataset I am wrangling is the tweet archive of twitter user WeRateDogs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tasks of this projects are:\n",
    "- Gathering Data\n",
    "- Assessing Data\n",
    "- Cleaning Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gathering Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We were required to gather the data from 3 different sources:\n",
    "\n",
    "- Twitter Archive file:\n",
    "    > For all 2500+ of their tweets as of August 1, 2017, this archive contains the fundamental tweet information (tweet ID, timestamp, text, etc.).\n",
    "    \n",
    "    > Udacity provided the twitter-archive-enhanced.csv, which was manually downloaded and imported into a pandas data frame.\n",
    "\n",
    "- The tweet image predictions:\n",
    "    >The most accurate dog breed predictions for each image in the Twitter archive are included in this file. The most confident prediction's picture number, tweet ID, image URL, and top predictions are all included in the dataset.\n",
    "    \n",
    "    > The data is programmatically downloaded into a tsv file from the URK address using the requests library. The image-predictions.tsv file's content is imported into a pandas data frame ten times.\n",
    "    \n",
    "- Twitter API File:\n",
    "    > The tweet ID, favorite count, and retweet count are all contained here. Udacity provided the data, which was manually downloaded before being loaded from the tweet-json.txt file into a pandas data frame.\n",
    "\n",
    "    \n",
    "    > A different approach would have involved seeking authorization from Twitter and utilizing the tweepy Python library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assessing Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examining the data programmatically and visually is described as assessing data. Visual examination mostly entails observing the data's structure, which is a significant drawback when working with massive data. Yet, two criteria—**Quality** and **Tidiness**—were used to access the databases.\n",
    "\n",
    "\n",
    "Quality has to do with the content of the data. This includes completeness, validity, accuracy and consistency of a dataset. I was able to identify missing records,incorrect data-type and incorrect data-entries in the different datasets.\n",
    "\n",
    "Tidiness has to do with the structure of the data.I fixed the wrong columns in these datasets and made sure they were explicit enough for analysis in order to verify the data was clean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas is used to clean up the quality and order issues found in the section on assessing data:\n",
    "\n",
    "This is thought of as the process's last phase in data wrangling. It includes the phases of define, code, and test.\n",
    "\n",
    "Before offering answers, I made a clone of these datasets, made sure the appropriate Python packages and libraries were utilized successfully programmatically, and then combined the copies into a single dataframe. I identified the answers to the numerous quality and order issues, developed the programs, and tested the solutions by making sure they were applied correctly.\n",
    "\n",
    "The cleaned dataframe was then given the name \"twitter archive master\" by me. Additionally, this was leveraged to produce high-quality insights, in-depth analysis, and visualization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "dc07d24e2f18896857f0b2a651fe84ba40ce7b297e58d8804a308c8039f752a6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
